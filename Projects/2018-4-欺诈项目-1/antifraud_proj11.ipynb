{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project1 量化金融信用评估实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import sklearn.preprocessing as sklp\n",
    "import sklearn.decomposition as skld\n",
    "import sklearn.manifold as sklm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_df: (98, 86)\n"
     ]
    }
   ],
   "source": [
    "dir_prefix = \"/home/ubuntu/workspace/data_set_public/public_data/\"\n",
    "train_dataset = \"project_LoanStats3d_securev1.csv\"\n",
    "sample_dataset = \"project_2015Sample.csv\"\n",
    "sample_df = pd.read_csv(dir_prefix + sample_dataset)\n",
    "train_df = pd.read_csv(dir_prefix + train_dataset, low_memory=False) # please note that this is a large file, it may take a little bit longer to read\n",
    "sample_df.head()\n",
    "#print 'Shape of train_df:', sample_df.shape\n",
    "test_dataset = \"antifraud_proj1_test.csv\"\n",
    "test_df = pd.read_csv(dir_prefix + test_dataset, low_memory=False)\n",
    "#print 'Shape of test_df:', test_df.shape\n",
    "#print 'Which fields are missing in the test dataset?'\n",
    "#print set(sample_df.columns) - set(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 针对Train_df数据进行清理\n",
    "- 这里将Train_df作为函数的输入\n",
    "### 2.1 对各个非数值字段进行数值化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#删除train_df字段中['open_il_6m', 'fico_range_low', 'member_id', 'loan_status', 'fico_range_high', 'id']\n",
    "train_df = train_df.drop(['open_il_6m','fico_range_low','member_id','fico_range_high','id'],axis = 1)\n",
    "#修改因变量字段loan_status为数字\n",
    "train_df = train_df[(train_df['loan_status']=='Current') | (train_df['loan_status']=='Charged Off') | (train_df['loan_status']=='Fully Paid')]\n",
    "train_df['loan_status'] = train_df['loan_status'].replace(['Current','Fully Paid','Charged Off',],[0,0,1])\n",
    "\n",
    "#将因变量放在第一位\n",
    "train_df_loan_status = train_df.loan_status\n",
    "train_df = train_df.drop('loan_status',axis = 1)\n",
    "train_df.insert(0,'loan_status',train_df_loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Numeralization_df(train_df):\n",
    "    del train_df['Unnamed: 0']\n",
    "    train_keys = list(train_df.keys())\n",
    "    lenth = len(train_df)\n",
    "    train_df['term'] = train_df['term'].replace([' 60 months',' 36 months'],[1,0])#'term' 修改为0-1变量\n",
    "    train_df = train_df.drop(['issue_d','emp_title'], axis = 1)#删除issue_d，emp_title\n",
    "    train_df['emp_length']=train_df['emp_length'].replace(['1 year', '10+ years', '2 years', '3 years', '4 years',\n",
    "                                     '5 years', '6 years', '7 years', '8 years', '9 years', \n",
    "                                     '< 1 year', 'n/a'],[1,10,2,3,4,5,6,7,8,9,0,0])\n",
    "    train_df['home_ownership'] = train_df['home_ownership'].replace(['MORTGAGE', 'RENT', 'OWN','ANY','NONE'],[0,0.5,1,0.5,0])\n",
    "    train_df['verification_status'] = train_df['verification_status'].replace(['Source Verified', \n",
    "                                                                                 'Not Verified', 'Verified'],\n",
    "                                                                               [0.5,0,1])\n",
    "    train_df.index = range(len(train_df))\n",
    "    #修改因变量为数字\n",
    "    del train_df['pymnt_plan']#数据值只有“n”，故删除\n",
    "    del train_df['desc']#描述\n",
    "    train_df['purpose'] = train_df['purpose'].replace(['car','credit_card','debt_consolidation','educational',\n",
    "                                                         'home_improvement','house','major_purchase','medical',\n",
    "                                                         'moving','other','renewable_energy','small_business',\n",
    "                                                         'vacation','wedding'],[0.6,0.2,1.0,0.2,0.4,0.4,0.6,0.6,0.4,0.2,0.6,0.8,0.2,0.4])\n",
    "    del train_df['title']#和‘purpose’字段一致\n",
    "    del train_df['zip_code']#借贷程序的zip code 前三位\n",
    "\n",
    "    #根据贷款各项金额大小按比例排序\n",
    "    #地区,我按照各地的人均GDP与总GDP，按照7:3的比例进行合计评分，并归一化\n",
    "    train_df['addr_state'] = train_df['addr_state'].replace(['ID','DC', 'TX', 'PA', 'GA', 'FL', 'NY', 'CA', 'TN', 'KS', 'MA', \n",
    "                                                               'RI','OH', 'OR', 'HI', 'SC', 'MD', 'AZ', 'WI', 'VA', 'CO', \n",
    "                                                               'IN', 'LA','NC', 'NJ', 'MO', 'NM', 'IL', 'MI', 'SD', 'WA', \n",
    "                                                               'NH', 'VT', 'AL','MN', 'CT', 'DE', 'NE', 'WV', 'MT', 'NV',\n",
    "                                                               'OK', 'WY', 'AR', 'KY','MS', 'ME', 'UT', 'ND', 'AK'],\n",
    "                                                              [0.125305614,0.249494545,0.065555092,0.039895621,0.085444756,0.593361556,\n",
    "                                                                  0.190277765,0.278189313,1,0.227317672,0.192702231,0.155311576,\n",
    "                                                                  0.131447429,0.280336943,0.132540357,0.106452086,0.069307246,\n",
    "                                                                  0.15253082,0.302445071,0.21218416,0.030257468,0.137946485,\n",
    "                                                                  0.208228197,0.110989561,0,0.048207504,0.16335986,0.231921874,\n",
    "                                                                  0.161796013,0.117910048,0.2777898,0.050846682,0.094623649,\n",
    "                                                                  0.494175602,0.191919255,0.085791932,0.160341405,0.220064923,\n",
    "                                                                  0.111679786,0.046895947,0.10685987,0.109896491,0.441591068,\n",
    "                                                                  0.097945954,0.207944043,0.06716049,0.242232775,0.141853116,\n",
    "                                                                  0.013158262,0.213121431])\n",
    "\n",
    "    train_df['revol_util'] = train_df['revol_util'].str.strip('%').astype(float)/100\n",
    "    #网上将百分比转化的方法：p_float = df['p_str'].str.strip(\"%\").astype(float)/100\n",
    "    #sample_df['revol_util'] \n",
    "\n",
    "    #对earliest_cr_line进行数字化处理\n",
    "    train_df_keys = list(train_df.keys())\n",
    "    lenth = len(train_df)\n",
    "    j = 0\n",
    "    for i in list(train_df.keys()):\n",
    "        if i =='earliest_cr_line':\n",
    "            break\n",
    "        j += 1\n",
    "    #print j,train_df_keys[j]\n",
    "\n",
    "    dic = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,\n",
    "           'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "    month = [dic[i[0:3]]for i in train_df['earliest_cr_line']]\n",
    "    if test_df['earliest_cr_line'][0][-3] == '-':\n",
    "        year = [int(i[-2:]) for i in train_df['earliest_cr_line']]\n",
    "        for i in range(lenth):\n",
    "            if year[i]<20:\n",
    "                year[i] += 2000\n",
    "            else:\n",
    "                year[i] += 1900\n",
    "    else :\n",
    "        year = [int(i[-4:]) for i in train_df['earliest_cr_line']]\n",
    "    num_month = pd.DataFrame([(2018-year[i])*12+4-month[i] for i in range(lenth)])\n",
    "    del train_df['earliest_cr_line']\n",
    "    train_df.insert(j,'earliest_cr_line',num_month)\n",
    "    \n",
    "#    print '字段数字化完成,如下字段进行数字化：\\n',['term','emp_length',\n",
    "#                                  'home_ownership','verification_status','purpose',\n",
    "#                                  'addr_state','revol_util','earliest_cr_line']\n",
    "#    print '如下字段被删除：\\n',['Unnamed: 0','issue_d','emp_title','pymnt_plan','desc','title','zip_code']\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2找寻含有nan的字段，直接删除超过50%空缺的字段，并用中位数填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_non_from_df(train_df):\n",
    "    train_df_keys = list(train_df.keys())\n",
    "    lenth = len(train_df)\n",
    "    non_num = [[train_df_keys[i],(lenth - int(train_df[train_df_keys[i]].describe()[0]))]for i in range(len(train_df_keys))]\n",
    "    need_to_del_non_name = [non_num[i] for i in range(len(train_df_keys))if non_num[i][1]>lenth/2.5]\n",
    "    train_df = train_df.drop([need_to_del_non_name[i][0]for i in range(len(need_to_del_non_name))],axis = 1)\n",
    "    train_df_keys = list(train_df.keys())\n",
    "    non_num = [[train_df_keys[i],(lenth - int(train_df[train_df_keys[i]].describe()[0]))]for i in range(len(train_df_keys))]\n",
    "#    print '予以删除的字段及空缺值数量：\\n',need_to_del_non_name\n",
    "\n",
    "    need_to_solve_non = [non_num[i] for i in range(len(non_num)) if non_num[i][1]>0]\n",
    "    names_to_solve = [need_to_solve_non[i][0] for i in range(len(need_to_solve_non))]\n",
    "#    print '需要填充空缺的字段及其空缺数量：\\n',need_to_solve_non,'\\n它们已用字段中位数填充。\\n'\n",
    "\n",
    "    for i in range(len(need_to_solve_non)):\n",
    "        mean = train_df[need_to_solve_non[i][0]].describe()['50%']\n",
    "        train_df[names_to_solve[i]] = train_df[names_to_solve[i]].fillna(mean)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 字段异常值检测及处理\n",
    "经过分析，仅有字段‘dti’，‘revol_util'，’bc_util'适合进行异常值处理:\n",
    "    \n",
    "- 而'dti','revol_util','bc_util'是比率值，范围应在0~1或0~100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Abnormal_solve_df(train_df):\n",
    "    train_keys = list(train_df.keys())\n",
    "    lenth = len(train_df)\n",
    "    num_abnormal = [0]*3\n",
    "    name_abnormal = ['dti','revol_util','bc_util']\n",
    "    ab_index = [0]*3\n",
    "    j = 0\n",
    "    for i in range(len(train_keys)):\n",
    "        if train_keys[i]==name_abnormal[j]:\n",
    "            ab_index[j] = i\n",
    "            j += 1\n",
    "        if j == 3:\n",
    "            break\n",
    "    #将revol_util的数x100，转换为百分数\n",
    "    train_df['revol_util'] = [train_df['revol_util'][i]*100 for i in range(lenth)]\n",
    "\n",
    "    for i in range(3):#排除了第一列，应变量\n",
    "        mean = train_df[name_abnormal[i]].describe()['50%']\n",
    "        train_df = train_df.sort_values(by = name_abnormal[i])\n",
    "        train_df.index = range(lenth)\n",
    "        data_append = list(train_df[name_abnormal[i]])\n",
    "        j = 0\n",
    "        while((train_df[name_abnormal[i]][j]<0) or train_df[name_abnormal[i]][lenth-1-j]>100):\n",
    "            if train_df[name_abnormal[i]][j]<0:\n",
    "                num_abnormal[i] += 1\n",
    "                data_append[j] = mean\n",
    "            if train_df[name_abnormal[i]][lenth-1-j]>100:\n",
    "                num_abnormal[i] += 1\n",
    "                data_append[lenth-1-j] = mean\n",
    "            j += 1\n",
    "        del train_df[name_abnormal[i]]\n",
    "        train_df.insert(ab_index[i],name_abnormal[i],pd.DataFrame(data_append))\n",
    "    #print ab_index\n",
    "#    print 'dti,revol_util,bc_util字段分别有如下数量异常值被处理,以中位数填充：\\n',[[name_abnormal[i],num_abnormal[i]]for i in range(len(name_abnormal))]\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 根据清理后的数据，进行归一化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Z-score化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_score_df(train_df):\n",
    "    train_keys = list(train_df.keys())\n",
    "    variable = train_df.iloc[:,1:]\n",
    "    Z_score = sklp.StandardScaler().fit(variable)\n",
    "    \n",
    "    Z_score_train_df = pd.DataFrame(Z_score.transform(variable))\n",
    "    Z_score_train_df.insert(0,'loan_status',train_df['loan_status'])\n",
    "    Z_score_train_df.columns = train_keys\n",
    "#    print '数据集进行Z_score归一化处理完毕\\n'\n",
    "#Z_score_train_df.to_csv('Z_score_train_df.csv')\n",
    "    return Z_score_train_df,Z_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 最小-最大标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Min_Max_df(cl_train_df):\n",
    "    cl_train_keys = list(cl_train_df.keys())\n",
    "    variable = cl_train_df.iloc[:,1:]\n",
    "    Min_Max = sklp.MinMaxScaler().fit(variable)\n",
    "    Min_Max_train_df = Min_Max.transform(variable)\n",
    "\n",
    "    Min_Max_train_df = pd.DataFrame(Min_Max_train_df)\n",
    "    Min_Max_train_df.insert(0,'loan_status',cl_train_df['loan_status'])\n",
    "    Min_Max_train_df.columns = cl_train_keys\n",
    "#    print '数据集进行Min_Max归一化处理完毕\\n'\n",
    "    #Min_Max_train_df.to_csv('Min_Max_train_df.csv')\n",
    "    return Min_Max_train_df,Min_Max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 正则化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Normalize_df(cl_train_df):\n",
    "    cl_train_keys = list(cl_train_df.keys())\n",
    "    variable = cl_train_df.iloc[:,1:]\n",
    "    Normalize = sklp.Normalizer().fit(variable)\n",
    "    Normalize_train_df = Normalize.transform(variable)\n",
    "\n",
    "    Normalize_train_df = pd.DataFrame(Normalize_train_df)\n",
    "    Normalize_train_df.insert(0,'loan_status',cl_train_df['loan_status'])\n",
    "    Normalize_train_df.columns = cl_train_keys\n",
    "#    print '数据集进行正则化处理完毕\\n'\n",
    "    #Normalize_train_df.to_csv('Normalize_train_df.csv')\n",
    "    return Normalize_train_df,Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字段数字化完成,如下字段进行数字化：\n",
      "['term', 'emp_length', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'revol_util', 'earliest_cr_line']\n",
      "如下字段被删除：\n",
      "['Unnamed: 0', 'issue_d', 'emp_title', 'pymnt_plan', 'desc', 'title', 'zip_code']\n",
      "予以删除的字段及空缺值数量：\n",
      "[['mths_since_last_delinq', 196926], ['mths_since_last_record', 333779], ['open_acc_6m', 384509], ['open_il_12m', 384509], ['open_il_24m', 384509], ['mths_since_rcnt_il', 385052], ['total_bal_il', 384509], ['il_util', 387175], ['open_rv_12m', 384509], ['open_rv_24m', 384509], ['max_bal_bc', 384509], ['all_util', 384509], ['inq_fi', 384509], ['total_cu_tl', 384509], ['inq_last_12m', 384509], ['mths_since_recent_bc_dlq', 301121], ['mths_since_recent_revol_delinq', 259695]]\n",
      "需要填充空缺的字段及其空缺数量：\n",
      "[['revol_util', 156], ['bc_open_to_buy', 3765], ['bc_util', 4013], ['mo_sin_old_il_acct', 11837], ['mths_since_recent_bc', 3614], ['mths_since_recent_inq', 43442], ['num_rev_accts', 1], ['num_tl_120dpd_2m', 18365], ['percent_bc_gt_75', 4029]] \n",
      "它们已用字段中位数填充。\n",
      "\n",
      "数据集进行Z_score归一化处理完毕\n",
      "\n",
      "数据集进行正则化处理完毕\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = Numeralization_df(train_df)\n",
    "\n",
    "train_df = Find_non_from_df(train_df)\n",
    "\n",
    "train_df = Abnormal_solve_df(train_df)\n",
    "\n",
    "Z_train_df,Z_score = Z_score_df(train_df)\n",
    "M_train_df,Min_Max = Min_Max_df(train_df)\n",
    "N_train_df,Normalize = Normalize_df(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 考虑到test_df与train_df字段统一性，因此对test_df删除一些字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字段数字化完成,如下字段进行数字化：\n",
      "['term', 'emp_length', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'revol_util', 'earliest_cr_line']\n",
      "如下字段被删除：\n",
      "['Unnamed: 0', 'issue_d', 'emp_title', 'pymnt_plan', 'desc', 'title', 'zip_code']\n",
      "予以删除的字段及空缺值数量：\n",
      "[]\n",
      "需要填充空缺的字段及其空缺数量：\n",
      "[['dti', 30], ['revol_util', 50], ['bc_open_to_buy', 954], ['bc_util', 994], ['mo_sin_old_il_acct', 2520], ['mths_since_recent_bc', 914], ['mths_since_recent_inq', 11045], ['num_tl_120dpd_2m', 4444], ['percent_bc_gt_75', 956]] \n",
      "它们已用字段中位数填充。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = Numeralization_df(test_df)\n",
    "test_df = test_df.drop(['mths_since_last_delinq','mths_since_last_record',\n",
    "                        'open_acc_6m','open_il_12m', 'open_il_24m',\n",
    "                        'mths_since_rcnt_il', 'total_bal_il', 'il_util',\n",
    "                        'open_rv_12m', 'open_rv_24m','max_bal_bc',\n",
    "                        'all_util', 'inq_fi','total_cu_tl', \n",
    "                        'inq_last_12m', 'mths_since_recent_bc_dlq', \n",
    "                        'mths_since_recent_revol_delinq'],axis = 1)\n",
    "test_df = Find_non_from_df(test_df)\n",
    "test_df = Abnormal_solve_df(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### test_df 的归一化使用train_df中归一化的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_keys = list(test_df.keys())\n",
    "Z_test_df = pd.DataFrame(Z_score.transform(test_df))\n",
    "Z_test_df.columns = test_df_keys\n",
    "\n",
    "M_test_df = pd.DataFrame(Min_Max.transform(test_df))\n",
    "M_test_df.columns = test_df_keys\n",
    "\n",
    "N_test_Df = pd.DataFrame(Normalize.transform(test_df))\n",
    "N_test_Df.columns = test_df_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.数据降维—— PCA\n",
    "\n",
    "- 清理后的数据为Z_train_df和Z_test_df(以Z-score归一数据集为例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncrementalPCA(batch_size=4050, copy=True, n_components=None, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable = Z_train_df.iloc[:,1:]\n",
    "train_df_keys = list(train_df.keys())\n",
    "lenth = len(variable)\n",
    "pca = skld.IncrementalPCA(n_components=None,batch_size=lenth/100)\n",
    "pca.fit(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 变换后特征向量的特征值占比累计统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95%占比的特征向量数量： 35\n"
     ]
    }
   ],
   "source": [
    "score = [i*100 for i in pca.explained_variance_ratio_]\n",
    "for i in range(len(score)-1):\n",
    "    score[i+1] += score[i]\n",
    "    if score[i]>95:\n",
    "        dim = i+1\n",
    "        break\n",
    "#print '95%占比的特征向量数量：',dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选取dim维特征向量,形成pca_train_df和pca_test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_df = pd.DataFrame(pca.transform(variable)).iloc[:,:dim]\n",
    "pca_train_df_keys = ['']*dim\n",
    "for i in range(dim):\n",
    "    pca_train_df_keys[i] = 'X'+str(i+1)\n",
    "pca_train_df.columns = pca_train_df_keys\n",
    "pca_train_df.insert(0,'Y',Z_train_df['loan_status'])\n",
    "pca_test_df = pd.DataFrame(pca.transform(Z_test_df)).iloc[:,:dim]\n",
    "pca_test_df_keys = ['']*dim\n",
    "for i in range(dim):\n",
    "    pca_test_df_keys[i] = 'X'+str(i+1)\n",
    "pca_test_df.columns = pca_test_df_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pca_train_df.to_csv('pca_train_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 生成样本集\n",
    "- SMOTEENN\n",
    "- EasyEnsemble\n",
    "- BalanceCascade\n",
    "\n",
    "参考blog：https://blog.csdn.net/kizgel/article/details/78553009?locationNum=6&fps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.1 SMOTE\n",
    "- SMOTE方法容易产生较多噪声，因此选用smoteenn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#切分pca_train_df数据集为训练与测试\n",
    "X_train,X_test,y_train,y_test = train_test_split(pca_train_df.iloc[:,1:],pca_train_df['Y'],test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DataFramize(X,y,X_keys):\n",
    "    X,y = pd.DataFrame(X),pd.DataFrame(y)\n",
    "    X.columns = X_keys\n",
    "    y.columns = ['y']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X,y = X_train,y_train\n",
    "X_sm,y_sm = SMOTE().fit_sample(X,y)\n",
    "X_sm=np.round(X_sm,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算时间太久，已经存入文件‘train_smoteenn.csv'\n",
    "#from imblearn.combine import SMOTEENN\n",
    "#X_smn,y_smn = SMOTEENN().fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smote = pd.DataFrame(X_sm)\n",
    "train_smote.columns = list(pca_train_df.keys())[1:]\n",
    "train_smote.insert(0,'Y',y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_smote.to_csv('train_smote.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 EasyEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom imblearn.ensemble import EasyEnsemble\\nee = EasyEnsemble(random_state=0, n_subsets=15)\\nX_e,y_e = ee.fit_sample(X, y)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from imblearn.ensemble import EasyEnsemble\n",
    "ee = EasyEnsemble(random_state=0, n_subsets=15)\n",
    "X_e,y_e = ee.fit_sample(X, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 BalancedCascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom imblearn.ensemble import BalanceCascade\\nfrom sklearn.linear_model import LogisticRegression\\nbc = BalanceCascade(random_state=0,n_max_subset=10,estimator=LogisticRegression(random_state=0))\\nX_bc,y_bc = bc.fit_sample(X,y)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from imblearn.ensemble import BalanceCascade\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "bc = BalanceCascade(random_state=0,n_max_subset=10,estimator=LogisticRegression(random_state=0))\n",
    "X_bc,y_bc = bc.fit_sample(X,y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型建立\n",
    "- Logist 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64901383642\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "#使用smote后的训练集X_sm,y_sm\n",
    "LR = LogisticRegression(C = 0.1,penalty='l2',solver='sag',class_weight='balanced')\n",
    "LR.fit(X_sm,y_sm)\n",
    "#print LR.score(X_sm,y_sm)\n",
    "LRy_predict_sm = LR.predict(X_test)\n",
    "LRy_prob_sm = LR.predict_proba(X_test)\n",
    "#print X_test\n",
    "#print np.array(LRy_predict_sm)\n",
    "LRy_prob_sm1=LRy_prob_sm[:,1]\n",
    "##print LRy_prob_sm1\n",
    "#print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_sm,y_sm)\n",
    "knn_predict_sm = LR.predict(X_test)\n",
    "knn_prob_sm = LR.predict_proba(X_test)\n",
    "knn_prob_sm1=LRy_prob_sm[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print metrics.accuracy_score(y_test,LRy_predict_sm)\n",
    "#print metrics.accuracy_score(y_test,LRy_predict_smn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ensemble-logist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#easyensemble\\xe7\\x9a\\x84\\xe5\\xad\\x90\\xe9\\x9b\\x86\\xe6\\x95\\xb0\\xe9\\x87\\x8f\\neasy_num = 15\\ny_test_prob = np.array([0.0]*len(test_df))\\nfor i in range(easy_num):\\n    LR.fit(X_e[i],y_e[i])\\n    y_test_prob += LR.predict_proba(pca_test_df)[:,1]\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#easyensemble的子集数量\n",
    "easy_num = 15\n",
    "y_test_prob = np.array([0.0]*len(test_df))\n",
    "for i in range(easy_num):\n",
    "    LR.fit(X_e[i],y_e[i])\n",
    "    y_test_prob += LR.predict_proba(pca_test_df)[:,1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_prob = y_test_prob/easy_num\n",
    "#y_test_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#RF\\xe6\\xa8\\xa1\\xe5\\x9e\\x8b\\xef\\xbc\\x8c\\xe4\\xbd\\xbf\\xe7\\x94\\xa8smote\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\nfrom sklearn.ensemble import RandomForestClassifier \\nRF=RandomForestClassifier(n_estimators=40,max_features=6,n_jobs=16,max_depth=6,min_samples_leaf=5000)\\nRF.fit(X_sm,y_sm)\\nRF.score(X_sm,y_sm)\\nRF_predict_sm = RF.predict(X_test)\\nRF_prob_sm = RF.predict_proba(X_test)\\n#print \"Accuracy : %.4g\" % metrics.accuracy_score(y_test, RF_predict_sm)\\n#print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, RF_predict_sm)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#RF模型，使用smote数据\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "RF=RandomForestClassifier(n_estimators=40,max_features=6,n_jobs=16,max_depth=6,min_samples_leaf=5000)\n",
    "RF.fit(X_sm,y_sm)\n",
    "RF.score(X_sm,y_sm)\n",
    "RF_predict_sm = RF.predict(X_test)\n",
    "RF_prob_sm = RF.predict_proba(X_test)\n",
    "#print \"Accuracy : %.4g\" % metrics.accuracy_score(y_test, RF_predict_sm)\n",
    "#print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, RF_predict_sm)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建GBM模型，使用SMOTE后的训练集X_sm,y_sm\n",
    "min_samples_split_init=int(0.0075*y_sm.shape[0])\n",
    "LGBM=GradientBoostingClassifier(random_state=10,min_samples_split=min_samples_split_init,n_estimators=50,min_samples_leaf=5000,max_depth=6)\n",
    "LGBM.fit(X_sm,y_sm)\n",
    "LGBM.score(X_sm,y_sm)\n",
    "LGBMy_predict_sm = LGBM.predict(X_test)\n",
    "LGBMy_prob_sm = LGBM.predict_proba(X_test)\n",
    "#print \"Accuracy : %.4g\" % metrics.accuracy_score(y_test, LGBMy_predict_sm)\n",
    "#print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, LGBMy_predict_sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#逻辑回归、RF和GBM的整合\n",
    "#sumary_prob_sm=(LRy_prob_sm1+LGBMy_prob_sm[:,1]+RF_prob_sm[:,1]+knn_prob_sm1)/4\n",
    "sumary_predict_sm=LRy_predict_sm+LGBMy_predict_sm+knn_predict_sm\n",
    "sumary_predict_sm.tolist()\n",
    "lengh=len(sumary_predict_sm)\n",
    "for xx in range(1,lengh):\n",
    "    if sumary_predict_sm[xx]<2:\n",
    "        sumary_predict_sm[xx]=0\n",
    "    else:\n",
    "        sumary_predict_sm[xx]=1\n",
    "sumary_predict_sm\n",
    "#sumary_predict_sm=[x=1 for x in sumary_predict_sm if x>=2]\n",
    "#sumary_prob_sm\n",
    "#print \"Accuracy : %.4g\" % metrics.accuracy_score(y_test, sumary_prob_sm)\n",
    "#print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, sumary_prob_sm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the results\n",
    "You do not need to modify the following lines. \n",
    "\n",
    "**Please do not modify the file name \"antifraud_proj1_result.csv\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_predict = LR.predict_proba(pca_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_prediction_test = np.squeeze(d['Y_prediction_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Predict': sumary_predict_sm}\n",
    "test_predict = pd.DataFrame(data=d)\n",
    "test_predict.to_csv(\"antifraud_proj1_result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
